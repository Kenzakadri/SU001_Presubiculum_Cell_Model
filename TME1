import pandas as pd
import numpy as np
import scipy.stats as stats
from sklearn.datasets import make_classification
import matplotlib.pyplot as plt
import statsmodels.stats.multitest as smm
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import make_gaussian_quantiles


#############Exploratory analysis

#1.
print("1.")
df1 = pd.read_table("BreastDiagnosis",delimiter=",",header=None,index_col=0)
df2 = pd.ExcelFile("Data_Cortex_Nuclear.xls")
df2 = pd.read_excel(df2,index_col=0)

print("df1")
print(df1)

print("df2")
print(df2)


#2.

print("2.")

df1.fillna(df1.median(),inplace=True)
df2.fillna(df2.median(),inplace=True)


#3.

print("3.")

df1_M=df1.loc[df1.iloc[:,0]=='M',:]
df1_M=df1_M.drop(df1_M.columns[0],axis=1)
df1_B=df1.loc[df1.iloc[:,0]=='B',:]
df1_B=df1_B.drop(df1_B.columns[0],axis=1)


df2_ctr=df2.loc[df2['Genotype']=='Control',:]
del df2_ctr['Genotype']
del df2_ctr['Treatment']
del df2_ctr['Behavior']
del df2_ctr['class']
df2_Ts65Dn=df2.loc[df2['Genotype']=='Ts65Dn',:]
del df2_Ts65Dn['Genotype']
del df2_Ts65Dn['Treatment']
del df2_Ts65Dn['Behavior']
del df2_Ts65Dn['class']

# La suppression de ces colonnes permet de faire les tests statistiques avec seulement les colonnes avec des données numériques.


print("df1B")
print(df1_B)
print("df1M")
print(df1_M)

print("df2ctr")
print(df2_ctr)
print("df2ts65Dn")
print(df2_Ts65Dn)


#4

print("4.")
corr = []
def matrice (mat) :
    mrange = len(mat.columns)
    Mt = np.zeros((mrange, mrange))
    P = np.zeros((mrange,mrange))
    for i in (range(mrange)):
        for j in (range(mrange)):
            if type(mat.iloc[0,i]) != str and type(mat.iloc[0,j]) != str : #Cette ligne peut être supprimée après avoir retiré directement les colonnes avec des données qualitatives. Ceci est la boucle initiale pour ignorer les colonnes
                [x,y] = stats.pearsonr(mat.iloc[:,i],mat.iloc[:,j])
                Mt[i,j] = x
                Mt[j,i] = x
                if x > 0.5 or x < -0.5 and y < 0.05:
                    P[i,j] = x
                    P[j,i] = x # Pour faire l'autre sens de la matrice
                else :
                    P[i,j] = 0
                    P[j,i] = 0
    print(Mt)
    print("test")
    print(P)
    a = np.array(P) >0 # True = fortement corrélée  (plus de  0.5 ou moins de  -0.5 avec une p_value < 0.05
    #print(a)
    print("Nombre de variables fortement corréllée?", np.sum(a/2)) # Compte le nombre de True, et divise ce nombre par deux pour éviter les répétitions
    return "fin"

print(matrice(df2))
print(matrice(df1))


def correlation_matrix(df):
    from matplotlib import pyplot as plt
    from matplotlib import cm as cm

    fig = plt.figure()
    ax1 = fig.add_subplot(111)
    cmap = cm.get_cmap('jet', 30)
    cax = ax1.imshow(df.corr(), interpolation="nearest", cmap=cmap)
    ax1.grid(True)
    plt.title('Corr coef')
    ticks = np.arange(0,len(df.columns),1)
    ax1.set_xticks(ticks)
    ax1.set_yticks(ticks)
    ax1.set_xticklabels(df.columns)
    ax1.set_yticklabels(df.columns)
    # Add colorbar, make sure to specify tick locations to match desired ticklabels
    fig.colorbar(cax, ticks=[0.5,0.6,.75,.8,.85,.90,.95,1])
    plt.show()
    return "end"

correlation_matrix(df1)
correlation_matrix(df2)
#Cela permet de visualiser plus clairement le tableau de corrélation

print("Un signe de corrélation négatif montre une connection entre les 2 variables de la même manière qu'une corrélation positive. Le signe de la corrélation permet de voir le sens de cette dernière. \n Une corrélation positive signifie que les deux variables sont corrélées dans le même sens, quand var1 augmente, var2 augmente également \n tandis qu'une corrélation négative signifie l'inverse (quand var1 augmente var2 diminue de façon corrélée")


#5.


print("5.")
print("L'énoncé demande de faire un test de wilcoxon (stats.wilcoxon()), Or les données n'étant pas appareillé l'une a l'autre, aussi nous avons deux possibilités : \n - faire un test de mann-whitney \n - faire un wilcoxon_ranksum\. Ces deux tests sont équivalents ")


def mn (mat1,mat2) :
    method = ['b', 's', 'hs', 'h', 'sh', 'hommel', 'fdr_i', 'fdr_n', 'fdr_tsbh', 'fdr_tsbky', 'fdr_gbs'] #On teste toutes les méthodes
    col = len(mat1.columns)
    J = np.empty((col,2))
    for i in range(0,col):
        [x,y] = stats.mannwhitneyu(mat1.iloc[:,i],mat2.iloc[:,i])
        J[i][0] = x
        J[i][1] = y
    for names in method:
        print(names)
        multi = smm.multipletests(J[:,1],alpha=0.05,method=names)[:2]
        print(multi[:2])
    return J

print("df2")
print(mn(df2_ctr,df2_Ts65Dn))
print("df1")
print(mn(df1_B,df1_M))
print("vérification = ",stats.mannwhitneyu(df1_M.iloc[:,4],df1_B.iloc[:,4]))



#6.
print("6.")

print("Il faut choisir le type de méthode le plus rigoureux. \n Les méthodes qui reposent le FDR (fraction de faux négatifs attendus / tous les rejets) sont plus rigoureuses que les methodes de type FWER.\n Le test de fdr_tsbh semble être le plus rigoureux et le plus puissant de la liste. \n Le test le moins rigoureux semble être le fdr_bh. Parmis les methodes qui utilisent le FWER, Hommel semble être la plus appropriées ")

print("7")

def ttest (mat1,mat2) :
    col = len(mat1.columns)
    J = np.empty((col,2))
    ttestcorlist = []
    name = list(mat1)
    for i in range(0,col):
        [x,y] = stats.ttest_ind(mat1.iloc[:,i],mat2.iloc[:,i])
        J[i][0] = x
        J[i][1] = y
    print(J)
    ttestcor =smm.multipletests(J[:,1], alpha=0.05,method="fdr_tsbh")
    a = 0
    for y in ttestcor[0]:
        if y == True : # on compte le nombre de fois ou H0 est rejeté (donc distribution différentes) après correction
            a+=1
            ttestcorlist.append(name[a])
    print(("variables différentes = ", ttestcorlist))
    print("nombre de variables différentes = ",a)
    return "fin"

print("df2ctrl, df2Ts65Dn")
print(ttest(df2_ctr,df2_Ts65Dn))
print("df1B,df1M")
print(ttest(df1_B,df1_M))

# print("8.")

#boxplot
names_df1=list(df1_M)



for i in range(0,len(df1_M.columns)):
    ax=plt.subplot(5, 6, i+1)
    box_plot_data=[df1_M.iloc[:,i],df1_B.iloc[:,i]]
    ax.set_title(str(names_df1[i]))
    ax.boxplot(box_plot_data,labels=['M','B'])

plt.show()


names_df2=list(df2_ctr)



for i in range(0,len(df2_Ts65Dn.columns)):
    ax=plt.subplot(8, 10, i+1)
    box_plot_data=[df2_ctr.iloc[:,i],df2_Ts65Dn.iloc[:,i]]
    ax.set_title(str(names_df2[i]))
    ax.boxplot(box_plot_data,labels=['ctr','Ts65Dn'])

plt.show()

print("Pour comparer les boxplots aux valeurs du t-test, il est indispensable de discriminer les rejets de H0. On remarque que d'après les résultats il y a : \n - 48 variables statistiquements différentes pour les groupes de df2 \n - 27 variables statistiquement différentes pour les groupes de df1. \n Les données ne sont pas toujours cohérentes, par exemple la variable PKCA_N semble entraîner un rejet de H0 d'après le t-test, pourtant sur les boxplots les variables ne semblent pas être différentes.")

#9.
print("Les tests de correction de p_value semble être indispensables dans l'étude de plusieurs p_value répétées comme c'est le cas ici. Cependant, les tests demandés par l'enoncé ne sont pas adéquats aux données que l'on possède. D'une part, sans la compréhension des données elles-mêmes il est difficile de choisir le test le plus approprié (par exemple la première colonne de df1 correspond au nom des souris, sans le savoir on aurait pu faire un test deçu \n ceci n'étant pas indiqué clairement, cela peut entraîner des erreurs. \n De plus, il faut répondre à certaines exigeances pour utiliser des tests paramétriques, tel que l'homoscédasticité et la normalité des données, ce qui n'est pas le cas ici. \n \n Les variables les plus significatives pour discriminer les deux groupes sont celles ayant des p_value corrigées au test de student  les plus basses. \n  Concernant le seuil de significativité alpha, il est en général = 0.05, on a 5% de chance de se tromper. \n Les hypothèses d'ajustement devraient être celles qui permettent de vérifier les tests en amont. Les données n'étant ici\n  pas normales, on pourrait faire des corrections des tests (correction post-hoc) après ces derniers. On peut également comme précédemment corriger les p_value avec un test tel que fdr_tsdh.")

################ Adaboost

print("Adaboost")

print("1. ")
X, y = make_classification(n_samples=1000, n_features=4,
                               n_informative=2, n_redundant=0,
                               random_state=0, shuffle=False)

clf = AdaBoostClassifier(n_estimators=100, random_state=0)
print(clf.fit(X, y))
print(clf.feature_importances_)

print(clf.predict([[0, 0, 0, 0]]))
print(clf.score(X, y)  )

print("2.")
print("n_estimator = le nombre de weak learners. Cela permet d'entrainer le modèle, ce nombre doit être différent de 0 et est égale par défaut à 50")

print("n_estimators = 1")
clf = AdaBoostClassifier(n_estimators=1, random_state=0)
print(clf.fit(X, y))
print(clf.feature_importances_)

print(clf.predict([[0, 0, 0, 0]]))
print(clf.score(X, y)  )

print("n_estimators = 500")
clf = AdaBoostClassifier(n_estimators=500, random_state=0)
print(clf.fit(X, y))
print(clf.feature_importances_)

print(clf.predict([[0, 0, 0, 0]]))
print(clf.score(X, y)  )

print("n_estimators = 1000")
clf = AdaBoostClassifier(n_estimators=1000, random_state=0)
print(clf.fit(X, y))
print(clf.feature_importances_)

print(clf.predict([[0, 0, 0, 0]]))
print(clf.score(X, y)  )


print("n_estimators = 10000")
clf = AdaBoostClassifier(n_estimators=10000, random_state=0)
print(clf.fit(X, y))
print(clf.feature_importances_)

print(clf.predict([[0, 0, 0, 0]]))
print(clf.score(X, y)  )

print("Plus on a de weak learners et plus on entraîne le modèle pour que ces weak learners deviennent des strong learners. \nLes weak learners sont des 'algorithmes' peu performants, avec environ 50% de chance de se tromper (probabilité légèrement supérieure à celle du hasard). Plus il y a de weak learners et plus le modèle s'améliore. \n Ainsi on atteind logiquement un plateau quand le nombre de weak learner est suffisant, le modèle ne pouvant pas apprendre à l'infini. Le score de décision permet quant à lui de voir la 'pureté' des classes, dans cet exemple, un score de -0.2 signifie être dans la classe A")



# Construct dataset
X1, y1 = make_gaussian_quantiles(cov=2.,
                                 n_samples=200, n_features=2,
                                 n_classes=2, random_state=1)
X2, y2 = make_gaussian_quantiles(mean=(3, 3), cov=1.5,
                                 n_samples=300, n_features=2,
                                 n_classes=2, random_state=1)
X = np.concatenate((X1, X2))
y = np.concatenate((y1, - y2 + 1))

# Create and fit an AdaBoosted decision tree
bdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),
                         algorithm="SAMME",
                         n_estimators=200)

bdt.fit(X, y)

plot_colors = "br"
plot_step = 0.02
class_names = "AB"

plt.figure(figsize=(10, 5))

# Plot the decision boundaries
plt.subplot(121)
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),
                     np.arange(y_min, y_max, plot_step))

Z = bdt.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)
plt.axis("tight")

# Plot the training points
for i, n, c in zip(range(2), class_names, plot_colors):
    idx = np.where(y == i)
    plt.scatter(X[idx, 0], X[idx, 1],
                c=c, cmap=plt.cm.Paired,
                s=20, edgecolor='k',
                label="Class %s" % n)
plt.xlim(x_min, x_max)
plt.ylim(y_min, y_max)
plt.legend(loc='upper right')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Decision Boundary')

# Plot the two-class decision scores
twoclass_output = bdt.decision_function(X)
plot_range = (twoclass_output.min(), twoclass_output.max())
plt.subplot(122)
for i, n, c in zip(range(2), class_names, plot_colors):
    plt.hist(twoclass_output[y == i],
             bins=10,
             range=plot_range,
             facecolor=c,
             label='Class %s' % n,
             alpha=.5,
             edgecolor='k')
x1, x2, y1, y2 = plt.axis()
plt.axis((x1, x2, y1, y2 * 1.2))
plt.legend(loc='upper right')
plt.ylabel('Samples')
plt.xlabel('Score')
plt.title('Decision Scores')

plt.tight_layout()
plt.subplots_adjust(wspace=0.35)
plt.show()

print("On voit qu'il y a deux classes, : \n - classe A \n - classe B \n. Il y a deux couleurs de délimitation :  bleu et orange. Ces délimitations sont faites à partir des weak learners.  \n Plus on aura d'iterations (de weak learners) et plus les délimitations seront précises. Par exemple, la première itérations délimites la zone supérieure gauche comme ne comportant pas de points rouge. Une seconde iteration permet d'affiner cette zone. Plus on fait d'iterations et plus on est précis \n Dans le cas du graphique on remarque que les délimitations ne sont linéaires. On remarque aussi par exemple que la partie en bas à gauche comporte plus d'objet de la classe B (délimitaion orange), aussi, si on trouve un point à ce niveau, on aura plus de chance que ce soit un objet de la classe B ")

print("4.")

print("L'Adaboost combine linéairement des conclusions hypothèse cependant les hypothèses elles-mêmes ne sont pas forcémentlinéaire, ainsi ce dernier n'est pas un classifieur linéaire. Le fait que les délimitations dans notre cas ne soient pas linéaires appuie notre conclusion. ")
